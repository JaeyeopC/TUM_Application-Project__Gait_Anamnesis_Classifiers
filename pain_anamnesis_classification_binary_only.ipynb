{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a16ea88a",
   "metadata": {},
   "source": [
    "# Pain Anamnesis Classification using Neural Networks\n",
    "\n",
    "This notebook implements a deep neural network for classifying pain symptoms based on biomechanical measurements. The model predicts multiple binary pain indicators across different body regions.\n",
    "\n",
    "## Setup and Data Preparation\n",
    "\n",
    "The following cell:\n",
    "1. Imports required libraries\n",
    "2. Loads and preprocesses the pain anamnesis data\n",
    "3. Prepares features and targets for model training\n",
    "\n",
    "### Data Structure:\n",
    "- **Input Features**: 7 biomechanical measurements including:\n",
    "  - Left/Right movement deviation averages\n",
    "  - Left/Right resting deviation averages\n",
    "  - Left/Right step averages\n",
    "  - Shoe size\n",
    "- **Target Variables**: Binary pain indicators (0/1) for:\n",
    "  - Various foot regions (forefoot, midfoot, heel)\n",
    "  - Upper limb regions (wrist, elbow, fingers, upper arm, thumb, forearm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cb56e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Load the Excel file (make sure you have openpyxl installed: pip install openpyxl)\n",
    "data = pd.read_excel(\"colored_columns_output_filtered.xlsx\")  # Replace with your file path\n",
    "\n",
    "# Define the 7 input columns.\n",
    "input_cols = [\n",
    "    \"SchiefstandBewegungMmDurchschnitt_links\",\n",
    "    \"SchiefstandBewegungMmDurchschnitt_rechts\",\n",
    "    \"SchiefstandRuheMmDurchschnitt_links\",\n",
    "    \"SchiefstandRuheMmDurchschnitt_rechts\",\n",
    "    \"AuftrittDurchschnitt_links\",\n",
    "    \"AuftrittDurchschnitt_rechts\",\n",
    "    \"Schuhgröße\"\n",
    "]\n",
    "\n",
    "# Define the original binary target columns.\n",
    "binary_target_cols = [\n",
    "    \"Schmerz_Vorfuß_Links\", \"Schmerz_Vorfuß_Rechts\", \n",
    "    \"Schmerz_Mittelfuß_Links\", \"Schmerz_Mittelfuß_Rechts\",\n",
    "    \"Schmerz_Ferse_Links\", \"Schmerz_Ferse_Rechts\",\n",
    "    \"Schmerz_Handgelenk_links\", \"Schmerz_Handgelenk_rechts\",\t\n",
    "    \"Schmerz_Ellenbogen_links\", \"Schmerz_Ellenbogen_rechts\",\t\n",
    "    \"Schmerz_Finger_links\", \"Schmerz_Finger_rechts\",\t\n",
    "    \"Schmerz_Oberarm_links\", \"Schmerz_Oberarm_rechts\",\t\n",
    "    \"Schmerz_Daumen_links\", \"Schmerz_Daumen_rechts\",\t\n",
    "    \"Schmerz_Unterarm_links\", \"Schmerz_Unterarm_rechts\"\n",
    "]\n",
    "\n",
    "# Identify ordinal columns (all \"Schmerz_*\" columns not in the binary list).\n",
    "all_schmerz_cols = [col for col in data.columns if col.startswith(\"Schmerz_\")]\n",
    "ordinal_target_cols = [col for col in all_schmerz_cols if col not in binary_target_cols]\n",
    "\n",
    "# Clean data.\n",
    "data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "data.dropna(subset=ordinal_target_cols, inplace=True)\n",
    "\n",
    "# Convert original binary targets to 0/1.\n",
    "for col in binary_target_cols:\n",
    "    data[col] = data[col].map({True: 1, 'TRUE': 1, 'True': 1,\n",
    "                               False: 0, 'FALSE': 0, 'False': 0})\n",
    "\n",
    "# Ensure ordinal targets are integers.\n",
    "for col in ordinal_target_cols:\n",
    "    data[col] = data[col].astype(int)\n",
    "\n",
    "# Convert ordinal targets to binary: set to 0 if value is 0–3, else 1.\n",
    "for col in ordinal_target_cols:\n",
    "    data[col] = (data[col] > 3).astype(int)\n",
    "\n",
    "# Combine original binary targets and the converted ordinal targets.\n",
    "all_binary_target_cols = binary_target_cols + ordinal_target_cols\n",
    "\n",
    "# Extract features and combined binary targets.\n",
    "X = data[input_cols].values.astype(np.float32)\n",
    "y_all = data[all_binary_target_cols].values.astype(np.float32)\n",
    "\n",
    "# Split data into train (70%), validation (15%), and test (15%)\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y_all, test_size=0.15, random_state=42\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.1765, random_state=42\n",
    ")  # ≈15% for validation\n",
    "\n",
    "# Scale the input features.\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcafbe3",
   "metadata": {},
   "source": [
    "## Custom Dataset Implementation\n",
    "\n",
    "The `PainDataset` class handles data management for PyTorch training:\n",
    "\n",
    "1. **Features**:\n",
    "   - Converts numpy arrays to PyTorch tensors\n",
    "   - Scales input features using StandardScaler\n",
    "   - Handles batch processing\n",
    "\n",
    "2. **Labels**:\n",
    "   - Converts multiple pain indicators to binary format\n",
    "   - Manages multi-target classification\n",
    "\n",
    "3. **Data Loading**:\n",
    "   - Implements required PyTorch Dataset methods\n",
    "   - Enables efficient batch processing during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5c2c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PainDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)  # binary targets\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Create dataset instances.\n",
    "train_dataset = PainDataset(X_train, y_train)\n",
    "val_dataset   = PainDataset(X_val, y_val)\n",
    "test_dataset  = PainDataset(X_test, y_test)\n",
    "\n",
    "# Create DataLoaders.\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c37d63",
   "metadata": {},
   "source": [
    "## Neural Network Architecture\n",
    "\n",
    "The `BinaryModel` class implements a deep neural network with:\n",
    "\n",
    "1. **Structure**:\n",
    "   - 7 hidden layers with batch normalization\n",
    "   - Decreasing layer sizes: 128 → 128 → 64 → 64 → 32 → 32 → 16\n",
    "   - Multiple binary outputs for pain classification\n",
    "\n",
    "2. **Features**:\n",
    "   - LeakyReLU activation functions\n",
    "   - Dropout regularization (0.2)\n",
    "   - Batch normalization for stable training\n",
    "   - Multi-target binary classification output\n",
    "\n",
    "3. **Design Choices**:\n",
    "   - Deep architecture for complex pattern recognition\n",
    "   - Regularization to prevent overfitting\n",
    "   - Batch normalization for faster convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdb5520",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_targets):\n",
    "        super(BinaryModel, self).__init__()\n",
    "        # Define 7 hidden layers. Feel free to adjust the hidden sizes.\n",
    "        self.hidden1 = nn.Linear(input_dim, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.hidden2 = nn.Linear(128, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.hidden3 = nn.Linear(128, 64)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.hidden4 = nn.Linear(64, 64)\n",
    "        self.bn4 = nn.BatchNorm1d(64)\n",
    "        self.hidden5 = nn.Linear(64, 32)\n",
    "        self.bn5 = nn.BatchNorm1d(32)\n",
    "        self.hidden6 = nn.Linear(32, 32)\n",
    "        self.bn6 = nn.BatchNorm1d(32)\n",
    "        self.hidden7 = nn.Linear(32, 16)\n",
    "        self.bn7 = nn.BatchNorm1d(16)\n",
    "        # Output layer predicts all binary targets.\n",
    "        self.out = nn.Linear(16, num_targets)\n",
    "        # Define dropout (you may adjust dropout rates if needed).\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.bn1(self.hidden1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.bn2(self.hidden2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.bn3(self.hidden3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.bn4(self.hidden4(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.bn5(self.hidden5(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.bn6(self.hidden6(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.bn7(self.hidden7(x)))\n",
    "        x = self.dropout(x)\n",
    "        logits = self.out(x)\n",
    "        return logits\n",
    "\n",
    "num_targets = len(all_binary_target_cols)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BinaryModel(input_dim=7, num_targets=num_targets).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524c4d66",
   "metadata": {},
   "source": [
    "## Model Training Configuration\n",
    "\n",
    "The training process implements:\n",
    "\n",
    "1. **Loss Function**:\n",
    "   - Binary Cross-Entropy with Logits\n",
    "   - Handles multiple binary classifications simultaneously\n",
    "\n",
    "2. **Optimization**:\n",
    "   - AdamW optimizer with learning rate 0.001\n",
    "   - Learning rate scheduling with ReduceLROnPlateau\n",
    "   - Gradient clipping for stable training\n",
    "\n",
    "3. **Training Process**:\n",
    "   - 50 epochs with early stopping\n",
    "   - Validation-based model checkpointing\n",
    "   - Accuracy and loss monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aadda3b9-54ff-4319-8dda-bc46aacd159f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ap/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Loss: 0.6850 | Val Loss: 0.6554\n",
      "Train Acc: 0.5700 | Val Acc: 0.6700\n",
      "Validation loss improved, saving model.\n",
      "Epoch 2/50 | Train Loss: 0.6273 | Val Loss: 0.6044\n",
      "Train Acc: 0.6704 | Val Acc: 0.7392\n",
      "Validation loss improved, saving model.\n",
      "Epoch 3/50 | Train Loss: 0.5846 | Val Loss: 0.5642\n",
      "Train Acc: 0.7292 | Val Acc: 0.7629\n",
      "Validation loss improved, saving model.\n",
      "Epoch 4/50 | Train Loss: 0.5536 | Val Loss: 0.5408\n",
      "Train Acc: 0.7585 | Val Acc: 0.7808\n",
      "Validation loss improved, saving model.\n",
      "Epoch 5/50 | Train Loss: 0.5373 | Val Loss: 0.5235\n",
      "Train Acc: 0.7694 | Val Acc: 0.7840\n",
      "Validation loss improved, saving model.\n",
      "Epoch 6/50 | Train Loss: 0.5290 | Val Loss: 0.5108\n",
      "Train Acc: 0.7726 | Val Acc: 0.7855\n",
      "Validation loss improved, saving model.\n",
      "Epoch 7/50 | Train Loss: 0.5208 | Val Loss: 0.5055\n",
      "Train Acc: 0.7733 | Val Acc: 0.7855\n",
      "Validation loss improved, saving model.\n",
      "Epoch 8/50 | Train Loss: 0.5154 | Val Loss: 0.5016\n",
      "Train Acc: 0.7758 | Val Acc: 0.7856\n",
      "Validation loss improved, saving model.\n",
      "Epoch 9/50 | Train Loss: 0.5141 | Val Loss: 0.5025\n",
      "Train Acc: 0.7749 | Val Acc: 0.7858\n",
      "Epoch 10/50 | Train Loss: 0.5130 | Val Loss: 0.5004\n",
      "Train Acc: 0.7750 | Val Acc: 0.7856\n",
      "Validation loss improved, saving model.\n",
      "Epoch 11/50 | Train Loss: 0.5109 | Val Loss: 0.4982\n",
      "Train Acc: 0.7755 | Val Acc: 0.7856\n",
      "Validation loss improved, saving model.\n",
      "Epoch 12/50 | Train Loss: 0.5091 | Val Loss: 0.4969\n",
      "Train Acc: 0.7747 | Val Acc: 0.7851\n",
      "Validation loss improved, saving model.\n",
      "Epoch 13/50 | Train Loss: 0.5100 | Val Loss: 0.4956\n",
      "Train Acc: 0.7758 | Val Acc: 0.7857\n",
      "Validation loss improved, saving model.\n",
      "Epoch 14/50 | Train Loss: 0.5082 | Val Loss: 0.4949\n",
      "Train Acc: 0.7759 | Val Acc: 0.7851\n",
      "Validation loss improved, saving model.\n",
      "Epoch 15/50 | Train Loss: 0.5078 | Val Loss: 0.4946\n",
      "Train Acc: 0.7758 | Val Acc: 0.7862\n",
      "Validation loss improved, saving model.\n",
      "Epoch 16/50 | Train Loss: 0.5089 | Val Loss: 0.4947\n",
      "Train Acc: 0.7758 | Val Acc: 0.7858\n",
      "Epoch 17/50 | Train Loss: 0.5047 | Val Loss: 0.4951\n",
      "Train Acc: 0.7760 | Val Acc: 0.7860\n",
      "Epoch 18/50 | Train Loss: 0.5089 | Val Loss: 0.4936\n",
      "Train Acc: 0.7758 | Val Acc: 0.7865\n",
      "Validation loss improved, saving model.\n",
      "Epoch 19/50 | Train Loss: 0.5062 | Val Loss: 0.4927\n",
      "Train Acc: 0.7761 | Val Acc: 0.7860\n",
      "Validation loss improved, saving model.\n",
      "Epoch 20/50 | Train Loss: 0.5054 | Val Loss: 0.4939\n",
      "Train Acc: 0.7755 | Val Acc: 0.7853\n",
      "Epoch 21/50 | Train Loss: 0.5035 | Val Loss: 0.4931\n",
      "Train Acc: 0.7763 | Val Acc: 0.7855\n",
      "Epoch 22/50 | Train Loss: 0.5053 | Val Loss: 0.4932\n",
      "Train Acc: 0.7757 | Val Acc: 0.7855\n",
      "Epoch 23/50 | Train Loss: 0.5064 | Val Loss: 0.4935\n",
      "Train Acc: 0.7762 | Val Acc: 0.7854\n",
      "Epoch 24/50 | Train Loss: 0.5045 | Val Loss: 0.4940\n",
      "Train Acc: 0.7763 | Val Acc: 0.7859\n",
      "Epoch 25/50 | Train Loss: 0.5027 | Val Loss: 0.4943\n",
      "Train Acc: 0.7762 | Val Acc: 0.7861\n",
      "Epoch 26/50 | Train Loss: 0.5031 | Val Loss: 0.4932\n",
      "Train Acc: 0.7764 | Val Acc: 0.7860\n",
      "Epoch 27/50 | Train Loss: 0.5044 | Val Loss: 0.4939\n",
      "Train Acc: 0.7766 | Val Acc: 0.7853\n",
      "Epoch 28/50 | Train Loss: 0.5024 | Val Loss: 0.4940\n",
      "Train Acc: 0.7762 | Val Acc: 0.7851\n",
      "Epoch 29/50 | Train Loss: 0.5028 | Val Loss: 0.4938\n",
      "Train Acc: 0.7765 | Val Acc: 0.7851\n",
      "Epoch 30/50 | Train Loss: 0.5048 | Val Loss: 0.4937\n",
      "Train Acc: 0.7760 | Val Acc: 0.7850\n",
      "Epoch 31/50 | Train Loss: 0.5035 | Val Loss: 0.4945\n",
      "Train Acc: 0.7762 | Val Acc: 0.7860\n",
      "Epoch 32/50 | Train Loss: 0.5041 | Val Loss: 0.4940\n",
      "Train Acc: 0.7756 | Val Acc: 0.7860\n",
      "Epoch 33/50 | Train Loss: 0.5026 | Val Loss: 0.4940\n",
      "Train Acc: 0.7761 | Val Acc: 0.7854\n",
      "Epoch 34/50 | Train Loss: 0.5013 | Val Loss: 0.4940\n",
      "Train Acc: 0.7764 | Val Acc: 0.7852\n",
      "Epoch 35/50 | Train Loss: 0.5042 | Val Loss: 0.4938\n",
      "Train Acc: 0.7765 | Val Acc: 0.7854\n",
      "Epoch 36/50 | Train Loss: 0.5029 | Val Loss: 0.4936\n",
      "Train Acc: 0.7763 | Val Acc: 0.7859\n",
      "Epoch 37/50 | Train Loss: 0.5033 | Val Loss: 0.4936\n",
      "Train Acc: 0.7760 | Val Acc: 0.7851\n",
      "Epoch 38/50 | Train Loss: 0.5023 | Val Loss: 0.4938\n",
      "Train Acc: 0.7761 | Val Acc: 0.7858\n",
      "Epoch 39/50 | Train Loss: 0.5029 | Val Loss: 0.4936\n",
      "Train Acc: 0.7764 | Val Acc: 0.7852\n",
      "Epoch 40/50 | Train Loss: 0.5007 | Val Loss: 0.4940\n",
      "Train Acc: 0.7767 | Val Acc: 0.7854\n",
      "Epoch 41/50 | Train Loss: 0.5034 | Val Loss: 0.4943\n",
      "Train Acc: 0.7764 | Val Acc: 0.7858\n",
      "Epoch 42/50 | Train Loss: 0.5039 | Val Loss: 0.4941\n",
      "Train Acc: 0.7762 | Val Acc: 0.7855\n",
      "Epoch 43/50 | Train Loss: 0.5018 | Val Loss: 0.4938\n",
      "Train Acc: 0.7761 | Val Acc: 0.7851\n",
      "Epoch 44/50 | Train Loss: 0.5022 | Val Loss: 0.4936\n",
      "Train Acc: 0.7761 | Val Acc: 0.7850\n",
      "Epoch 45/50 | Train Loss: 0.5028 | Val Loss: 0.4941\n",
      "Train Acc: 0.7764 | Val Acc: 0.7855\n",
      "Epoch 46/50 | Train Loss: 0.5018 | Val Loss: 0.4938\n",
      "Train Acc: 0.7764 | Val Acc: 0.7854\n",
      "Epoch 47/50 | Train Loss: 0.5011 | Val Loss: 0.4938\n",
      "Train Acc: 0.7763 | Val Acc: 0.7856\n",
      "Epoch 48/50 | Train Loss: 0.5033 | Val Loss: 0.4941\n",
      "Train Acc: 0.7760 | Val Acc: 0.7852\n",
      "Epoch 49/50 | Train Loss: 0.5042 | Val Loss: 0.4939\n",
      "Train Acc: 0.7765 | Val Acc: 0.7856\n",
      "Epoch 50/50 | Train Loss: 0.5024 | Val Loss: 0.4938\n",
      "Train Acc: 0.7760 | Val Acc: 0.7855\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()  # Binary loss\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', \n",
    "                                                       factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "def binary_accuracy(preds, targets):\n",
    "    pred_labels = (torch.sigmoid(preds) > 0.5).float()\n",
    "    correct = (pred_labels == targets).float()\n",
    "    return correct.mean().item()\n",
    "\n",
    "\n",
    "# Cell 6: Training Loop with Validation and Gradient Clipping\n",
    "best_val_loss = float('inf')\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    total_train_acc = 0.0\n",
    "    num_train_samples = 0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(X_batch)\n",
    "        loss = criterion(logits, y_batch)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_size = X_batch.size(0)\n",
    "        total_train_loss += loss.item() * batch_size\n",
    "        total_train_acc += binary_accuracy(logits, y_batch) * batch_size\n",
    "        num_train_samples += batch_size\n",
    "        \n",
    "    avg_train_loss = total_train_loss / num_train_samples\n",
    "    avg_train_acc = total_train_acc / num_train_samples\n",
    "    \n",
    "    # Validation evaluation.\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    total_val_acc = 0.0\n",
    "    num_val_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            logits = model(X_batch)\n",
    "            loss = criterion(logits, y_batch)\n",
    "            batch_size = X_batch.size(0)\n",
    "            total_val_loss += loss.item() * batch_size\n",
    "            total_val_acc += binary_accuracy(logits, y_batch) * batch_size\n",
    "            num_val_samples += batch_size\n",
    "            \n",
    "    avg_val_loss = total_val_loss / num_val_samples\n",
    "    avg_val_acc = total_val_acc / num_val_samples\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"Train Acc: {avg_train_acc:.4f} | Val Acc: {avg_val_acc:.4f}\")\n",
    "    \n",
    "    scheduler.step(avg_val_loss)\n",
    "    \n",
    "    # Save the model if validation loss improves.\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        print(\"Validation loss improved, saving model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78db6c88",
   "metadata": {},
   "source": [
    "## Model Inference and Evaluation\n",
    "\n",
    "This section demonstrates the model's prediction capabilities:\n",
    "\n",
    "1. **Inference Process**:\n",
    "   - Loads the best model state\n",
    "   - Processes test data\n",
    "   - Generates binary predictions\n",
    "\n",
    "2. **Output Format**:\n",
    "   - Binary predictions (0/1) for each pain indicator\n",
    "   - Probability thresholding at 0.5\n",
    "   - Multiple simultaneous predictions per input\n",
    "\n",
    "3. **Visualization**:\n",
    "   - Displays sample predictions\n",
    "   - Shows prediction format for clinical interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c28c086-41e2-4787-ade4-056be037947c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary predictions (first 5):\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    X_new = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    logits = model(X_new)\n",
    "    binary_preds = torch.sigmoid(logits)\n",
    "    # Convert probabilities to binary predictions.\n",
    "    binary_preds = (binary_preds > 0.5).float().cpu().numpy()\n",
    "    \n",
    "    print(\"Binary predictions (first 5):\")\n",
    "    print(binary_preds[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
